{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Regression\n",
    "Regression is the problem of predicting a (real-valued, not categorical) variable (target) given an $x$ unlabeled instance.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Given an element $x$ as set of attributes $(x_1,x_2,...x_n )$ the model aim to find a straight as:\n",
    "$$\n",
    "\\hat{y}=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n",
    "$$\n",
    "The values of the $\\beta_i$ are chosen so the error is minimized. Some error metrics are:\n",
    "\n",
    "* Root Mean Square Error: $RMSE=\\sqrt{\\frac{\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 }{N} }$\n",
    "* Mean Square Error: $MSE=\\frac{1}{N}\\sqrt{\\sum_{i=1}^N (\\hat{y}_i - y_i)^2  }$\n",
    "\n",
    "\n",
    "In order to find the values a \"gradient descent\" method can be used.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is used when the target variable is binary. So logistic regression is more like a classification method.\n",
    "It applies the sigmoid function $\\frac{1}{1+e^{-x}}$ to make the output be in the interval $(0,1)$.\n",
    "\n",
    "The logistic regression is modeled as:\n",
    "$$\n",
    "\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 +...+ \\beta_nx_n)} }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Classification (Supervised learning)\n",
    "In classification we aim to find to which class $c$ an element $x$ belong.\n",
    "\n",
    "\n",
    "### KNN\n",
    "Classify a new element with the class of the k nearest neighbor  elements.\n",
    "The distance can be euclidean, cosine distance, etc.\n",
    "\n",
    "### Decision tree\n",
    "A decision tree is a model in which an element is classified following the path from the root node to the leaf. In each node the decision to which branch to follow is made based on the threshold value of an attribute. The class of the element (or the probability of belonging)  is set when arrived to the leaf.\n",
    "\n",
    "Each node should test the most dominant attribute. For example, the ID3 tree split the decision based on the attribute with the most information gain (i.e. which attribute that, when split based on it, causes the entropy of the class to be reduced the most in the subsets).\n",
    "$$\n",
    "IG=H_D(C)-\\sum_{D' \\in D} P(D') \\cdot H_{D'}(C)\n",
    "$$\n",
    "where $H_D(C)$ is the entropy of the class $C$ in the dataset $D$.\n",
    "\n",
    "### Random forest\n",
    "\n",
    "Consist of using several decision trees, where each vote how to categorize a new element. They generalize better than a single tree but no belonging probability is retrieved.\n",
    "\n",
    "\n",
    "### Bayes classifier\n",
    "\n",
    "Bayes classifier is a probabilistic classifier, and so it retrieves the probability of an element belonging to a class.\n",
    "\n",
    "$$\n",
    "P(C|X)=\\frac{P(C) \\cdot P(X|C)}{P(X)} \\propto P(C) \\cdot P(X|C)\n",
    "$$\n",
    "\n",
    "which indicates the prior probability of the class multiplied by the likelihood of $X$ given $C$.\n",
    "\n",
    "Given an element $x$ as set of attributes $(x_1,x_2,...x_n )$ the probability of belonging to a class $c$ is:\n",
    "\n",
    "$$\n",
    "P(c) \\cdot P(X|c) = P(c) \\cdot \\prod_{x=1}^{n} P(x_i|c) \n",
    "$$\n",
    "\n",
    "For normalizing the probabilities:\n",
    "$$\n",
    "\\frac{P(c) \\cdot P(X|c)}{\\sum_{c' \\in C} P(c') \\cdot P(X|c')}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering (Unsupervised Learning)\n",
    "\n",
    "\n",
    "Problems in which data is unlabeled. Therefore there is no feedback mechanism.\n",
    "\n",
    "#### Elbow methods and silhouette\n",
    "[TO-DO]\n",
    "\n",
    "\n",
    "### K-means\n",
    "\n",
    "Clustering algorithm\n",
    "\n",
    "1. Choose k elements as centroids\n",
    "2. Repeat until convergence:\n",
    "\t- For each $x_i$:\n",
    "\t\t- Assign $x_i$ to the closest centroid\n",
    "\t- For each conglomerate:\n",
    "\t\t- Recompute the centroid of the conglomerate (the mean of the points).\n",
    "\n",
    "As it is sensitive to initial settings, it is better to run several runs.\n",
    "\n",
    "###  DBSCAN\n",
    "\n",
    "Density-based spatial clustering. It has two hyperparameters $\\epsilon$ and a threshold $t$.\n",
    "\n",
    "1. Assign any (unassigned) $x_i$  to a new cluster $c$.\n",
    "2. Identify the instances $x_{i,\\epsilon}$ within a distance $\\epsilon$. If there are more than $t$ instances, then assign them also to $c$.\n",
    "3.  For each $x_{i,\\epsilon}$: Repeat step 2.\n",
    "\n",
    "\n",
    "###  Hierarchical clustering\n",
    "\n",
    "This method clusters all the elements hierarchically, and it is afterwards when the number of cluster are decided. \n",
    "\n",
    "1. Initially, each element $x_i$ is a centroid.\n",
    "2. The closet centroids are joined. The middle point becomes the centroid of the cluster.\n",
    "3. Repeat step 2 until all elements are clustered.\n",
    "\n",
    "This creates a tree, and it can be decided afterwards in how many clusters to split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "\n",
    "In the Recurrent Neural Network section (RNN) we have seen how to transform a sequence of vectors to another sequence of vectors. However, both sequence are of the same length.\n",
    "\n",
    "When performing a translation is rare the case where the source and the target sentence have the same length. Therefore the Encoder-Decoder framework (Cho et al.,2014; Sutskever et al., 2014) was proposed to solve this problem. This consists of two RNNs, (e.g. LSTM or GRU) as follows:\n",
    "\n",
    "* Encoder: The encoder is an RNN that converts the sequence $x_1 , x-2 , ..., x_{T x}$ into a context vector c that summarizes the sentence. This vector c is the last output vector of the encoder.\n",
    "\n",
    "* Decoder: The decoder is an RNN that performs the inverse process. Given the context vector $c$ it generates a sequence of vectors $y_1 , y_2 , ..., y_{T y}$ (until the element EOS is found). Each $y_i$ is the vector corresponding to the i-th word of the translation.\n",
    "\n",
    "\n",
    "![Encoder_Decoder_Model](images/Encoder_Decoder_Model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq with Attention\n",
    "\n",
    "\n",
    "The problem of the encoder-decoder framework is that it encodes the whole sequence as a single vector $c$. This may not be enough for remembering the information of long sentences, and the decoder only has access to that single vector to generate the whole sequence of vectors. The Attention Model (Bahdanau et al., 2014) was introduced to solve this problem. \n",
    "\n",
    "Using this mechanism, instead of using a single fixed context vector $c$ to encode the input sequence, a different context vector $c_t$ is used at each step. This is created so the decoder identifies those parts of the input sequence that are relevant for generating the subsequent words. \n",
    "\n",
    "At step $t$ of the decoding, the vector $c_t$ is computed as a weighted sum of hidden states of the input sequence $h_{t}^{(s)}$ as:\n",
    "\n",
    "$$c_t=\\sum_{i=1}^{T_x} \\alpha_{t,i} \\cdot h_{i}^{(s)}$$\n",
    "\n",
    "The weight $\\alpha_{t,i}$ represents how much attention should be paid to the i-th word of the input sentence (the i-th output of the encoder $h_{i}^{(s)}$). \n",
    "\n",
    "This is computed as a softmax over an alignment score between the current target word $h_{t}^{(t)}$ and each of the words $h_{i}^{(s)}$ of the encoder:\n",
    "$$\n",
    "\\alpha_{t,i} = softmax_i(score(h_{t}^{(t)},h_{i}^{(s)}))\n",
    "$$\n",
    "The score can be computed in different ways, two of them are:\n",
    "* $score(h_{t}^{(t)},h_{i}^{(s)})=(h_{t}^{(t)})^{T} \\cdot W_1 \\cdot h_{i}^{(s)}$\n",
    "* $score(h_{t}^{(t)},h_{i}^{(s)})=v^{T}tanh(W_1 h_{t}^{(t)}+W_2 h_{i}^{(s)})$\n",
    "\n",
    "where $v$, $W_1$ and $W_2$ are matrices whose values are learned during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[\"Improving transductive data selection algorithms for machine translation\"](http://doras.dcu.ie/23726/1/thesis_AlbertoPoncelas.pdf)\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "A problem of the neural networks is that they can process inputs when they are vectors. However in some cases it is necessary to process a sequence of vectors as input. For example if a sentence is represented as a sequence of word-vectors.\n",
    "\n",
    "RNN are neural networks that form directed cyclic graphs. Having this structure allows the RNN to compute a sequence of vectors instead of a single vector.\n",
    "\n",
    "![RNN_1](images/RNN_1.png)\n",
    "\n",
    "\n",
    "This means that the input will not be a vector but a sequence of vectors $(x_1, x_2, ... x_{T})$. This sequence input is processed sequentially, providing a vector $x_t$ from the sequence at a time. At the step of processing $x_t$, the information of the previous output vector $h_{t-1}$ is also gathered. Then, the hidden state $h_{t}$ (and the output $y_{t}$) are produced. In the end, a sequence of vectors $(h_1, h_2,... h_{T})$ are generated.\n",
    "\n",
    "\n",
    "\n",
    "The representation of this structure can be unfolded as:\n",
    "\n",
    "\n",
    "![RNN_unwrap](images/RNN_unwrap.png)\n",
    "\n",
    "\n",
    "As each cell of the RNN has two inputs (the input $x_{t}$ and information of the previous hidden layer $h_{t-1}$), the equation of the neural network is extended as:\n",
    "$$h_t=f(W x_t + Uh_{t-1})$$\n",
    "\n",
    "where $W$ and $U$ are the weight matrices to be adjusted during training of the RNN (for simplification, we omit the bias).\n",
    "\n",
    "RNN obtain the information of the previous hidden state $h_{t-1}$ when processing $x_t$, which implicitly contains the information of the previous elements of the sequence. However, the long-distance dependencies are more difficult to learn the more the bigger the gap between $x_t$ and a previous element $x_{t-k}$ is. \n",
    "\n",
    "\n",
    "In order to solve this long-distance dependency problem LSTM and GRU were proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated recurrent unit (GRU)\n",
    "\n",
    "GRU introduces a gates to adjust the signal from the previous step. \n",
    "\n",
    "![GRU](images/GRU.png)\n",
    "\n",
    "In order to generate the output $h_t$,  we use a $gate_{update}$ to regulate how much information is taken from the previous step $h_{t-1}$ and how much from the new output candidate $\\tilde{h}_{t }$ as:\n",
    "\n",
    "$$h_t=(1-gate_{update}) \\cdot h_{t-1 } + gate_{update} \\cdot \\tilde{h}_{t }$$\n",
    "\n",
    "The candidate $\\tilde{h}_{t }$ is computed as:\n",
    "\n",
    "$$\\tilde{h}_{t }=tanh(W_{hx}X_{t}+W_{rh}(gate_{reset} \\cdot h_{t-1}) )$$\n",
    "\n",
    "which is dependent on the input $X_t$ and the previous hidden state $h_{t-1}$ (regulated by the reset gate $gate_{reset}$).\n",
    "\n",
    "Both $gate_{update}$ and $gate_{reset}$ are computed as small neural networks:\n",
    "\n",
    "* $gate_{update}=\\sigma(W_{ux}X_{t}+W_{uh}h_{t-1})$ \n",
    "* $gate_{reset}=\\sigma(W_{rx}X_{t}+W_{rh}h_{t-1})$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Long Short-Term Memory (LSMT)\n",
    "\n",
    "LSTM intoduced an additional signal $C_t$ which contains long-distance information. The purpose of this signal is carry information from past steps (as the inmediate previous step is already encoded by $h_{t-1}$). The particularity of LSTM is that it has three inputs and produce two outputs.\n",
    "\n",
    "\n",
    "![LSTM](images/LSTM.png)\n",
    "\n",
    "The inputs are the following three:\n",
    " * $X_t$: The input at current step $t$.\n",
    " * $h_{t-1}$: Hidden state of previous step (step $t-1$).\n",
    " * $C_{t-1}$: Cell state of previous step (step $t-1$).\n",
    "\n",
    "And it produced two outputs:\n",
    "* The cell state $C_t$ computed as:\n",
    "$$C_t=gate_{forget}\\cdot C_{t-1} + gate_{input} \\cdot tanh(W_{cx}X_{t}+W_{ch}h_{t-1})$$\n",
    "* The hidden state $h_t$ computed as:\n",
    "$$h_t=gate_{out} \\cdot tanh(C_t)$$\n",
    "\n",
    "The gates regulates different things such as: (i) how much of the previous cell state should be kept ($gate_{forget}$); (ii) how much of the new information should be added to the cell state ($gate_{input}$); and (iii) how much of the cell state should be included in the hidden state ($gate_{output}$). They are computed as small neural networks:\n",
    "\n",
    "* $gate_{forget}=\\sigma(W_{fx}X_{t}+W_{fh}h_{t-1})$ \n",
    "* $gate_{input}=\\sigma(W_{ix}X_{t}+W_{ih}h_{t-1})$\n",
    "* $gate_{output}=\\sigma(W_{ox}X_{t}+W_{oh}h_{t-1})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Improving Transductive Data Selection Algorithms for Machine Translation](http://doras.dcu.ie/23726/1/thesis_AlbertoPoncelas.pdf)\n",
    "\n",
    "https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
